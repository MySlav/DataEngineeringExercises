{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h1 style=\"font-size:30px;text-align: center;\">- Date Engineering  - Exercise 11 -</h1></b>\n",
    "<b><h1 style=\"font-size:25px;text-align: center;\">- Data Binning -</h1></b>\n",
    "\n",
    "<center><img src=\"https://www.algebra.hr/sveuciliste/wp-content/uploads/2023/11/algebra_UNIVERSITY-1-800x242.png\"/></center>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "<b>*Made: December 2024.* </b>\n",
    "\n",
    "<b>*Author: Mislav Spajić, mag. ing. comp.*</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TOvht7vqQGdR",
    "papermill": {
     "duration": 0.030335,
     "end_time": "2020-11-27T11:52:28.446538",
     "exception": false,
     "start_time": "2020-11-27T11:52:28.416203",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Binning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dB_j6LtTTO5j",
    "papermill": {
     "duration": 0.02728,
     "end_time": "2020-11-27T11:52:28.666647",
     "exception": false,
     "start_time": "2020-11-27T11:52:28.639367",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data binning (or bucketing) groups data in bins (or buckets), in the sense that it replaces values contained into a small interval with a single representative value for that interval. Sometimes binning improves accuracy in predictive models. \n",
    "\n",
    "Data binning is a type of data preprocessing, a mechanism which includes also dealing with [missing values](https://towardsdatascience.com/data-preprocessing-with-python-pandas-part-1-missing-data-45e76b781993), [formatting](https://towardsdatascience.com/data-processing-with-python-pandas-part-2-data-formatting-710c2eafa426), [normalization](https://towardsdatascience.com/data-preprocessing-with-python-pandas-part-3-normalisation-5b5392d27673) and [standardization](https://towardsdatascience.com/data-preprocessing-with-python-pandas-part-4-standardization-ccd5b1608f1c).\n",
    "\n",
    "Binning can be applied to convert numeric values to categorical or to sample (quantise) numeric values. \n",
    "* convert numeric to categorical includes binning by distance and binning by frequency\n",
    "* reduce numeric values includes quantisation (or sampling).\n",
    "\n",
    "Binning is a technique for data smoothing. Data smoothing is employed to remove noise from data. Three techniques for data smoothing:\n",
    "* binning\n",
    "* regression\n",
    "* outlier analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we exploit the `cupcake.csv` dataset, which contains the trend search of the word `cupcake` on Google Trends. Data are extracted from [this link](https://trends.google.com/trends/explore?q=%2Fm%2F03p1r4&date=all). We exploit the `pandas` library to import the dataset and we transform it into a dataframe through the `read_csv()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('cupcake.csv')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Equal Width Binning (Fixed-Width Binning):\n",
    "\n",
    "In this case we define the edges of each bin. We group values related to the column `Cupcake` into three groups: *small*, *medium* and *big*. \n",
    "In order to do it, we need to calculate the intervals within each group falls. We calculate the interval range as the difference between the maximum and minimum value and then we split this interval into three parts, one for each group.\n",
    "We exploit the functions `min()` and `max()` of dataframe to calculate the minimum value and the maximum value of the column `Cupcake`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_value = df['Cupcake'].min()\n",
    "max_value = df['Cupcake'].max()\n",
    "print(min_value)\n",
    "print(max_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can calculate the range of each interval, i.e. the minimum and maximum value of each interval. Since we have 3 groups, we need 4 edges of intervals (bins):\n",
    "* small - (edge1, edge2)\n",
    "* medium - (edge2, edge3)\n",
    "* big - (edge3, edge4)\n",
    "\n",
    "We can use the `linspace()` function of the `numpy` package to calculate the 4 bins, equally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "bins = np.linspace(min_value,max_value,4)\n",
    "bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['small', 'medium', 'big']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `cut()` function to convert the numeric values of the column `Cupcake` into the categorical values. We need to specify the bins and the labels. In addition, we set the parameter `include_lowest` to `True` in order to include also the minimum value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['bin_cut_linspace'] = pd.cut(df['Cupcake'], bins=bins, labels=labels, include_lowest=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the distribution of values, by using the `hist()` function of the `matplotlib` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(df['bin_cut_linspace'], bins=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can set the edges of each bin manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [ 0, 10, 50, 100 ]\n",
    "df['bin_cut_manual'] = pd.cut(df['Cupcake'] , bins=bins, labels=labels, include_lowest=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df['bin_cut_manual'], bins=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Equal Frequency Binning (Quantile Binning):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binning by frequency calculates the size of each bin so that each bin contains the (almost) same number of observations, but the bin range will vary. We can use the Python `pandas` `qcut()` function. We can set the `precision` parameter to define the number of decimal points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['bin_qcut'] = pd.qcut(df['Cupcake'], q=3,precision=1, labels=labels)\n",
    "plt.hist(df['bin_qcut'], bins=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot how data observations are transformed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling is another technique of data binning. It permits to reduce the number of samples, by grouping similar values or contiguous values.\n",
    "There are three approaches to perform sampling:\n",
    "* by bin means: each value in a bin is replaced by the mean value of the bin.\n",
    "* by bin median: each bin value is replaced by its bin median value.\n",
    "* by bin boundary: each bin value is replaced by the closest boundary value, i.e. maximum or minimum value of the bin.\n",
    "\n",
    "In order to perform sampling, the `binned_statistic()` function of the `scipy.stats` package can be used. This function receives two arrays as input, `x_data` and `y_data`, as well as the statistics to be used (e.g. median or mean) and the number of bins to be created. The function returns the values of the bins as well as the edges of each bin. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import binned_statistic\n",
    "x_data = np.arange(0, len(df))\n",
    "y_data = df['Cupcake']\n",
    "x_bins,bin_edges, misc = binned_statistic(y_data,x_data, statistic=\"mean\", bins=2)\n",
    "bin_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should approximate each value of the `df['Cupcake']` column to the median value of the corresponding bin. Thus we convert the bin edges to an `IntervalIndex`, which receives as index the left and right edges of each interval. In our case, the left edges starts from the beginning of the bin edges and do not contain the last value of the bin edges. The right edges instead, start from the second value of the bin edges and last until the last value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_intervals = pd.IntervalIndex.from_arrays(bin_edges[:-1], bin_edges[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can quantise the `Cupcake` column by defining a `set_to_median()` function which loops through the intervals and when it finds the correct interval, it returns the mid value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_to_median(x, bin_intervals):\n",
    "    for interval in bin_intervals:\n",
    "        if x in interval:\n",
    "            return interval.mid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `apply()` function to apply the `set_to_median()` to the `Cupcake` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sampled_cupcake'] = df['Cupcake'].apply(lambda x: set_to_median(x, bin_intervals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The scope of these changes made to\n",
    "# pandas settings are local to with statement.\n",
    "with pd.option_context('display.max_rows', None,\n",
    "                       'display.max_columns', None,\n",
    "                       'display.precision', 3,\n",
    "                       ):\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot results. We note the loss of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df['Cupcake'], label='original')\n",
    "plt.plot(df['sampled_cupcake'], color='red', label='sampled')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can plot the median values. We can calculate the `x` values (`x_bins`) corresponding to the binned values (`y_bins`) as the values at the center of the bin range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_bins = (bin_edges[:-1]+bin_edges[1:])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_data,y_data)\n",
    "plt.xlabel(\"X\"); \n",
    "plt.ylabel(\"Y\")\n",
    "\n",
    "plt.scatter(x_bins, y_bins,  color= 'red',linewidth=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Calculate the natural breaks in data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the package [jenkspy](https://pypi.org/project/jenkspy/), which contains a single function, called `jenks_breaks()`, which calculates the natural breaks of an array, exploiting the[ Fisher-Jenks algorithm](https://en.wikipedia.org/wiki/Jenks_natural_breaks_optimization). We can install the package by running `pip3 install jenkspy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jenkspy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jenkspy\n",
    "breaks = jenkspy.jenks_breaks(df['Cupcake'], n_classes=3)\n",
    "print(breaks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the `cut()` function to transform data in labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['bin_cut_break'] = pd.cut(df['Cupcake'] , bins=breaks, labels=labels, include_lowest=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can plot the histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df['bin_cut_break'], bins=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  5. Bibliography\n",
    "* Chris Moffit. [Binning Data with Pandas qcut and cut](https://pbpython.com/pandas-qcut-cut.html)\n",
    "* Chris Moffit. [Finding Natural Breaks in Data with the Fisher-Jenks Algorithm](https://pbpython.com/natural-breaks.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "beRJyQAezdX8",
    "papermill": {
     "duration": 0.029309,
     "end_time": "2020-11-27T11:52:30.797064",
     "exception": false,
     "start_time": "2020-11-27T11:52:30.767755",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "papermill": {
   "duration": 12.885345,
   "end_time": "2020-11-27T11:52:37.154623",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-11-27T11:52:24.269278",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
