{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<b><h1 style=\"font-size:30px;text-align: center;\">- Date Engineering  - Exercise 10 -</h1></b>\n",
    "<b><h1 style=\"font-size:25px;text-align: center;\">- Feature Scaling -</h1></b>\n",
    "\n",
    "<center><img src=\"https://www.algebra.hr/sveuciliste/wp-content/uploads/2023/11/algebra_UNIVERSITY-1-800x242.png\"/></center>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "<b>*Made: December 2024.* </b>\n",
    "\n",
    "<b>*Author: Mislav SpajiÄ‡, mag. ing. comp.*</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TOvht7vqQGdR",
    "papermill": {
     "duration": 0.030335,
     "end_time": "2020-11-27T11:52:28.446538",
     "exception": false,
     "start_time": "2020-11-27T11:52:28.416203",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dB_j6LtTTO5j",
    "papermill": {
     "duration": 0.02728,
     "end_time": "2020-11-27T11:52:28.666647",
     "exception": false,
     "start_time": "2020-11-27T11:52:28.639367",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature scaling is the process of normalising the range of features in a dataset. \n",
    "\n",
    "Real-world datasets often contain features that are varying in degrees of magnitude, range and units. Therefore, in order for machine learning models to interpret these features on the same scale, we have to perform feature scaling.\n",
    "\n",
    "In science, we all know the importance of comparing apples to apples and yet many people, especially beginners, have a tendency to overlook feature scaling as part of the preprocessing steps for machine learning. This has proven to cause models to make inaccurate predictions. \n",
    "\n",
    "In this tutorial, we will discuss why feature scaling is important, the difference between normalisation and standardisation as well as how feature scaling affects model accuracy. More specifically, we will explore the applications of 3 different types of scalers in the Scikit-learn library: \n",
    "\n",
    "1. [MixMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)\n",
    "2. [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n",
    "3. [RobustScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data visualisation\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'target']\n",
    "data = pd.read_csv('housing.csv', header=None, delimiter=r\"\\s+\", names=column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Examine data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values and data type\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hooray, no missing values!\n",
    "\n",
    "It also appears that all of our independent variables as well as the target variable are of the float64 data type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "\n",
    "data.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly observe that our features span across different range of values. This is largely attributed to the different units in which these features were measured and recorded.\n",
    "\n",
    "This is where feature scaling can help us solve this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Understand the effects of different scalers\n",
    "\n",
    "In this section, we will learn the distinction between normalisation and standardisation. Subsequently, we will look at the effects of 3 different feature scaling techniques in Scikit-learn. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 Theory\n",
    "\n",
    "Before we examine the effects of feature scaling, let us first go over some theories behind normalisation and standardisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.1 Normalisation\n",
    "\n",
    "Normalisation, also known as min-max scaling, is a scaling technique whereby the values in a column are shifted so that they are bounded between a fixed range of 0 and 1.\n",
    "\n",
    "X_new = (X - X_min) / (X_max - X_min)\n",
    "\n",
    "[MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) is the Scikit-learn function for normalisation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.2 Standardisation\n",
    "\n",
    "On the other hand, standardisation or Z-score normalisation is another scaling technique whereby the values in a column are rescaled so that they demonstrate the properties of a standard Gaussian distribution, that is mean = 0 and variance = 1. \n",
    "\n",
    "X_new = (X - mean) / std\n",
    "\n",
    "[StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) is the Scikit-learn function for standardisation.\n",
    "\n",
    "Unlike StandardScaler, [RobustScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html) scales features using statistics that are robust to outliers. More specifically, this scaler removes the median and scales the data according to the quantile range or by default, the interquartile range, thus making it less susceptible to outliers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.3 Normalisation vs standardisation \n",
    "\n",
    "The choice between normalisation or standardisation comes down to the application.\n",
    "\n",
    "Standardisation is generally preferred over normalisation in most machine learning context as it is especially important in order to compare the similarities between features based on certain distance measures. This is most prominent in Principal Component Analysis (PCA) where we are interested in the components that maximise the variance.\n",
    "\n",
    "Normalisation, on the other hand, also offers many practical applications particularly in computer vision and image processing where pixel intensities have to be normalised to fit within a the RGB colour range between 0 and 255. Furthermore, neural network algorithms typically require data to be normalised to a 0-1 scale before model training.  \n",
    "\n",
    "At the end of the day, there is no definitive answer as to whether you should normalise or standardise your data. One can always apply both techniques and compare the model performance for the best results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 Application \n",
    "\n",
    "Now that we have a theoretical understanding of feature scaling, let's see how they work in practice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictor and target variables\n",
    "X = data.drop('target', axis = 1)\n",
    "Y = data['target']\n",
    "\n",
    "# X, Y shape\n",
    "print(\"X shape: \", X.shape)\n",
    "print(\"Y shape: \", Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate MinMaxScaler, StandardScaler and RobustScaler\n",
    "\n",
    "norm = MinMaxScaler()\n",
    "standard = StandardScaler()\n",
    "robust = RobustScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MinMaxScaler\n",
    "normalised_features = norm.fit_transform(X)\n",
    "normalised_df = pd.DataFrame(normalised_features, index = X.index, columns = X.columns)\n",
    "\n",
    "# StandardScaler\n",
    "standardised_features = standard.fit_transform(X)\n",
    "standardised_df = pd.DataFrame(standardised_features, index = X.index, columns = X.columns)\n",
    "\n",
    "# RobustScaler\n",
    "robust_features = robust.fit_transform(X)\n",
    "robust_df = pd.DataFrame(robust_features, index = X.index, columns = X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate the effects of different scalers, I have chosen to examine the following features in our dataset before and after implementing feature scaling: \n",
    "\n",
    "- ZN\n",
    "- AGE\n",
    "- TAX\n",
    "- B "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots\n",
    "fig, ax = plt.subplots(2, 2, figsize = (12, 9))\n",
    "\n",
    "# Original\n",
    "sns.boxplot(x = 'variable', y = 'value', data = pd.melt(data[['ZN', 'AGE', 'TAX', 'B']]), ax = ax[0, 0])\n",
    "ax[0, 0].set_title('Original')\n",
    "ax[0, 0].set_xlabel('')\n",
    "ax[0, 0].set_ylabel('')\n",
    "\n",
    "# MinMaxScaler\n",
    "sns.boxplot(x = 'variable', y = 'value', data = pd.melt(normalised_df[['ZN', 'AGE', 'TAX', 'B']]), ax = ax[0, 1])\n",
    "ax[0, 1].set_title('MinMaxScaler')\n",
    "ax[0, 1].set_xlabel('')\n",
    "ax[0, 1].set_ylabel('')\n",
    "\n",
    "# StandardScaler\n",
    "sns.boxplot(x = 'variable', y = 'value', data = pd.melt(standardised_df[['ZN', 'AGE', 'TAX', 'B']]), ax = ax[1, 0])\n",
    "ax[1, 0].set_title('StandardScaler')\n",
    "ax[1, 0].set_xlabel('')\n",
    "ax[1, 0].set_ylabel('')\n",
    "\n",
    "# RobustScaler\n",
    "sns.boxplot(x = 'variable', y = 'value', data = pd.melt(robust_df[['ZN', 'AGE', 'TAX', 'B']]), ax = ax[1, 1])\n",
    "ax[1, 1].set_title('RobustScaler')\n",
    "ax[1, 1].set_xlabel('')\n",
    "ax[1, 1].set_ylabel('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, our original features have wildly different ranges.\n",
    "\n",
    "MinMaxScaler has rescaled our features so that their values are bounded between 0 and 1.\n",
    "\n",
    "StandardScaler and RobustScaler, on the other hand, have rescaled our features so that they are distributed around the mean of 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Compare model accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I mentioned in the introduction of this tutorial that unscaled data can adversely impact a model's ability to make accurate predictions but so far, we have not discussed exactly how and why they do. In fact, feature scaling does not always improve a model's performance. Some models do not require feature scaling. \n",
    "\n",
    "In this section, we will explore the following classes of machine learning algorithms and discuss whether or not feature scaling impact their performance:\n",
    "\n",
    "1. Gradient descent based algorithms\n",
    "2. Distance-based algorithms\n",
    "3. Tree-based algorithms "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1 Theory\n",
    "\n",
    "Let's first go over some concepts behind those algorithms and think about how and why feature scaling might be important to each of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1.1 Gradient descent based algorithms\n",
    "\n",
    "Gradient desent is an iterative optimisation algorithm that takes us to the minimum of a function. Machine learning algorithms like linear regression and logistic regression rely on gradient descent to minimise their loss functions or in other words, to reduce the error between the predicted values and the actual values. \n",
    "\n",
    "Having features with varying range of values will cause different step sizes for each feature. Therefore, to ensure that gradient descent converges more smoothly and quickly, we need to scale our features so that they have a similar scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1.2 Distance-based algorithms\n",
    "\n",
    "The underlying algorithms to distance-based models make them the most susceptible to unscaled data. \n",
    "\n",
    "Algorithms like k-nearest neighbours, support vector machines and k-means clustering use the distance between data points to determine their similarity. Hence, features with a greater magnitude will be given a higher weightage by the model. This is not an ideal scenario as we do not want our algorithm to be heavily biased towards a single feature.\n",
    "\n",
    "Evidently, it is important that we implement feature scaling to our data before fitting them to distance-based algorithms to ensure that all features contribute equally to the result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1.3 Tree-based algorithms \n",
    "\n",
    "Each node in a classification and regression trees (CART) model, otherwise known as decision trees represents a single feature in a dataset. The tree splits each node in such a way that it increases the homogeneity of that node. This split is not affected by the other features in the dataset. \n",
    "\n",
    "For that reason, we can conclude that decision trees are invariant to the scale of the features and therefore do not require feature scaling. This includes other ensemble models that are also tree-based such as random forest and gradient boosting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2 Proof of concept\n",
    "\n",
    "Now that we understand the types of models that are sensitive and insensitive to feature scaling, let us now convince ourselves with a concrete example using the Boston house prices dataset. \n",
    "\n",
    "Here, I have chosen 2 distance-based algorithms (KNN and SVR) as well as 1 tree-based algorithm (decision trees regressor) to predict the house prices.\n",
    "\n",
    "We should expect to see an improved model performance with feature scaling under KNN and SVR and a constant model performance under decision trees with and without feature scaling.\n",
    "\n",
    "Feel free to experiment with other types of models like linear regression, random forest and gradient boosting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate models \n",
    "knn = KNeighborsRegressor()\n",
    "svr = SVR()\n",
    "tree = DecisionTreeRegressor(max_depth = 10, random_state = 42)\n",
    "\n",
    "# Create a list which contains different scalers \n",
    "scalers = [norm, standard, robust]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, random_state = 42)\n",
    "\n",
    "print(\"X_train shape: \", X_train.shape)\n",
    "print(\"Y_train shape: \", Y_train.shape)\n",
    "print(\"X_test shape: \", X_test.shape)\n",
    "print(\"Y_test shape: \", Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get started, I think it is important to highlight the good practice of first fitting the scalers to the training set and then use that to transform the data in the test set. This is to prevent any data leakage and misleading accuracy scores.\n",
    "\n",
    "Here, I will construct a pipeline which contains a scaler and a model to fit and transform the features and subsequently make predictions using each model. The accuracy of these predictions are then evaluated using root mean squared error. The smaller the error, the better the model performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2.1 KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_rmse = []\n",
    "\n",
    "# Without feature scaling\n",
    "knn.fit(X_train, Y_train)\n",
    "pred = knn.predict(X_test)\n",
    "knn_rmse.append(np.sqrt(mean_squared_error(Y_test, pred)))\n",
    "\n",
    "# Apply different scaling techniques and make predictions using KNN \n",
    "for scaler in scalers:\n",
    "    pipe = make_pipeline(scaler, knn)\n",
    "    pipe.fit(X_train, Y_train)\n",
    "    pred = pipe.predict(X_test)\n",
    "    knn_rmse.append(np.sqrt(mean_squared_error(Y_test, pred)))\n",
    "\n",
    "# Show results     \n",
    "knn_df = pd.DataFrame({'RMSE': knn_rmse}, index = ['Original', 'MinMaxScaler', 'StandardScaler', 'RobustScaler'])\n",
    "knn_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the errors are much smaller with feature scaling than without feature scaling. In other words, our model performed better using scaled features. \n",
    "\n",
    "In this instance, KNN performed best under RobustScaler. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2.2 SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_rmse = []\n",
    "\n",
    "# Without feature scaling\n",
    "svr.fit(X_train, Y_train)\n",
    "pred = svr.predict(X_test)\n",
    "svr_rmse.append(np.sqrt(mean_squared_error(Y_test, pred)))\n",
    "\n",
    "# Apply different scaling techniques and make predictions using SVR\n",
    "for scaler in scalers:\n",
    "    pipe = make_pipeline(scaler, svr)\n",
    "    pipe.fit(X_train, Y_train)\n",
    "    pred = pipe.predict(X_test)\n",
    "    svr_rmse.append(np.sqrt(mean_squared_error(Y_test, pred)))\n",
    "\n",
    "# Show results\n",
    "svr_df = pd.DataFrame({'RMSE': svr_rmse}, index = ['Original', 'MinMaxScaler', 'StandardScaler', 'RobustScaler'])\n",
    "svr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to KNN, SVR also performed better with scaled features as seen by the lower errors.\n",
    "\n",
    "In this instance, SVR performed best under StandardScaler.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2.3 Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_rmse = []\n",
    "\n",
    "# Without feature scaling\n",
    "tree.fit(X_train, Y_train)\n",
    "pred = tree.predict(X_test)\n",
    "tree_rmse.append(np.sqrt(mean_squared_error(Y_test, pred)))\n",
    "\n",
    "# Apply different scaling techniques and make predictions using decision tree\n",
    "for scaler in scalers:\n",
    "    pipe = make_pipeline(scaler, tree)\n",
    "    pipe.fit(X_train, Y_train)\n",
    "    pred = pipe.predict(X_test)\n",
    "    tree_rmse.append(np.sqrt(mean_squared_error(Y_test, pred)))\n",
    "\n",
    "# Show results \n",
    "tree_df = pd.DataFrame({'RMSE': tree_rmse}, index = ['Original', 'MinMaxScaler', 'StandardScaler', 'RobustScaler'])\n",
    "tree_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, decision tree is insensitive to all feature scaling techniques as seen in the constant RMSE across scaled and unscaled features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarise, feature scaling is the process of transforming the features in a dataset so that their values share a similar scale.\n",
    "\n",
    "In this tutorial, we have learned the difference between normalisation and standardisation as well as 3 different scalers in the Scikit-learn library: MinMaxScaler, StandardScaler and RobustScaler. \n",
    "\n",
    "We also learned that gradient descent and distance-based algorithms require feature scaling while tree-based algorithms do not. We managed to prove this via an example with the Boston house prices dataset and comparing the model accuracy with and without feature scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Feature Scaling for Machine Learning: Understanding the Difference Between Normalization vs. Standardization](https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/)\n",
    "- [Normalization vs Standardization](https://www.geeksforgeeks.org/normalization-vs-standardization/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
