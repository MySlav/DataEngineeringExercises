{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h1 style=\"font-size:30px;text-align: center;\">- Date Engineering  - Exercise 7 -</h1></b>\n",
    "<b><h1 style=\"font-size:25px;text-align: center;\">- Exploratory Data Analysis in Python -</h1></b>\n",
    "\n",
    "<center><img src=\"https://www.algebra.hr/sveuciliste/wp-content/uploads/2023/11/algebra_UNIVERSITY-1-800x242.png\"/></center>\n",
    "\n",
    "=======================================================================================================\n",
    "\n",
    "\n",
    "<b>*Made: Novemeber 2024.* </b>\n",
    "\n",
    "<b>*Author: Mislav Spajić, mag. ing. comp.*</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TOvht7vqQGdR",
    "papermill": {
     "duration": 0.030335,
     "end_time": "2020-11-27T11:52:28.446538",
     "exception": false,
     "start_time": "2020-11-27T11:52:28.416203",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Exploratory Data Analysis in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dB_j6LtTTO5j",
    "papermill": {
     "duration": 0.02728,
     "end_time": "2020-11-27T11:52:28.666647",
     "exception": false,
     "start_time": "2020-11-27T11:52:28.639367",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8hg00soETQ3z",
    "papermill": {
     "duration": 0.027462,
     "end_time": "2020-11-27T11:52:28.721524",
     "exception": false,
     "start_time": "2020-11-27T11:52:28.694062",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**What is Exploratory Data Analysis ?**\n",
    "\n",
    "Exploratory Data Analysis or (EDA) is understanding the data sets by summarizing their main characteristics often plotting them visually. This step is very important especially when we arrive at modeling the data in order to apply Machine learning. Plotting in EDA consists of Histograms, Box plot, Scatter plot and many more. It often takes much time to explore the data. Through the process of EDA, we can ask to define the problem statement or definition on our data set which is very important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZfelutoyTS25",
    "papermill": {
     "duration": 0.02704,
     "end_time": "2020-11-27T11:52:28.776144",
     "exception": false,
     "start_time": "2020-11-27T11:52:28.749104",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**How to perform Exploratory Data Analysis ?**\n",
    "\n",
    "This is one such question that everyone is keen on knowing the answer. Well, the answer is it depends on the data set that you are working. There is no one method or common methods in order to perform EDA, whereas in this tutorial you can understand some common methods and plots that would be used in the EDA process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n3VfNkBBw15s",
    "papermill": {
     "duration": 0.027082,
     "end_time": "2020-11-27T11:52:28.830620",
     "exception": false,
     "start_time": "2020-11-27T11:52:28.803538",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**What data are we exploring today ?**\n",
    "\n",
    "\n",
    "\n",
    "Since I am a huge fan of cars, I got a very beautiful data-set of cars from Kaggle. The data-set can be downloaded from [here](https://www.kaggle.com/CooperUnion/cardataset). To give a piece of brief information about the data set this data contains more of 10, 000 rows and more than 10 columns which contains features of the car such as Engine Fuel Type, Engine HP, Transmission Type, highway MPG, city MPG and many more. So in this tutorial, we will explore the data and make it ready for modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQDO4JCqTThV",
    "papermill": {
     "duration": 0.027108,
     "end_time": "2020-11-27T11:52:28.885282",
     "exception": false,
     "start_time": "2020-11-27T11:52:28.858174",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FPSqz1wzTXvz",
    "papermill": {
     "duration": 0.027001,
     "end_time": "2020-11-27T11:52:28.939572",
     "exception": false,
     "start_time": "2020-11-27T11:52:28.912571",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. Importing the required libraries for EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9eLMx1Ebwa92",
    "papermill": {
     "duration": 0.027148,
     "end_time": "2020-11-27T11:52:28.994036",
     "exception": false,
     "start_time": "2020-11-27T11:52:28.966888",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Below are the libraries that are used in order to perform EDA (Exploratory data analysis) in this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GGyDovL2QDLa",
    "papermill": {
     "duration": 1.032787,
     "end_time": "2020-11-27T11:52:30.054137",
     "exception": false,
     "start_time": "2020-11-27T11:52:29.021350",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns                       #visualisation\n",
    "import matplotlib.pyplot as plt             #visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ye6eXuohTd5Q",
    "papermill": {
     "duration": 0.028147,
     "end_time": "2020-11-27T11:52:30.110492",
     "exception": false,
     "start_time": "2020-11-27T11:52:30.082345",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Y3Z2DbKTfJt",
    "papermill": {
     "duration": 0.027409,
     "end_time": "2020-11-27T11:52:30.165749",
     "exception": false,
     "start_time": "2020-11-27T11:52:30.138340",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. Loading the data into the data frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ko5zGJFCySaz",
    "papermill": {
     "duration": 0.027623,
     "end_time": "2020-11-27T11:52:30.221200",
     "exception": false,
     "start_time": "2020-11-27T11:52:30.193577",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Loading the data into the pandas data frame is certainly one of the most important steps in EDA, as we can see that the value from the data set is comma-separated. So all we have to do is to just read the CSV into a data frame and pandas data frame does the job for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0oVZnezwQ159",
    "outputId": "f1e0fe18-8fa0-482a-e2b9-2ecd87d97d9d",
    "papermill": {
     "duration": 0.104836,
     "end_time": "2020-11-27T11:52:30.409177",
     "exception": false,
     "start_time": "2020-11-27T11:52:30.304341",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"cars.csv\")\n",
    "# To display the top 5 rows \n",
    "df.head(5)               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fm-9dzdTRKpe",
    "outputId": "7892eaf7-0605-4b92-e139-cf0553041e51",
    "papermill": {
     "duration": 0.052534,
     "end_time": "2020-11-27T11:52:30.490899",
     "exception": false,
     "start_time": "2020-11-27T11:52:30.438365",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.tail(5) # To display the botton 5 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question to ask during the discovery phase:**\n",
    "1. How can I break this data into smaller groups so that I can understand it better?\n",
    "2. How can I prove my hypothesis?\n",
    "3. In its current form, can this data give me the answers I need?\n",
    "\n",
    "**Functions for data discovery:**\n",
    "\n",
    "| Function | Description |\n",
    "| ---- | ---- |\n",
    "| `DataFrame.head()` | The head() method will display the first n rows of the dataframe. <br> In the argument field, input the number of rows you want displayed in a Python notebook. The default is 5 rows. |\n",
    "| `DataFrame.info(X)` | The info() method will display a summary of the dataframe, including the range index, dtypes, column headers, and memory usage.<br> Leaving the argument field blank will return a full summary. As an option, in the argument field you can type in show_counts=True, which will return the count of non-null values for each column. |\n",
    "| `DataFrame.describe()` | The describe() method will return descriptive statistics of the entire dataset, including total count, mean, minimum, maximum, dispersion, and distribution. <br> Leaving the argument field blank will default to returning a summary of the data frame’s statistics. As an option, you can use “include=[X]” and “exclude=[X]” which will limit the results to specific data types, depending on what you input in the brackets. | \n",
    "| `DataFrame.shape` | shape is an attribute that returns a tuple representing the dimensions of the dataframe by number of rows and columns. Remember that attributes are not followed by parentheses. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JjQnr4SPzaL5",
    "papermill": {
     "duration": 0.029003,
     "end_time": "2020-11-27T11:52:30.549371",
     "exception": false,
     "start_time": "2020-11-27T11:52:30.520368",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vAmC369yTpMF",
    "papermill": {
     "duration": 0.029103,
     "end_time": "2020-11-27T11:52:30.608838",
     "exception": false,
     "start_time": "2020-11-27T11:52:30.579735",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3. Basic exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ESKxikIzA1d",
    "papermill": {
     "duration": 0.029261,
     "end_time": "2020-11-27T11:52:30.667608",
     "exception": false,
     "start_time": "2020-11-27T11:52:30.638347",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Here we check for the datatypes because sometimes the MSRP or the price of the car would be stored as a string, if in that case, we have to convert that string to the integer data only then we can plot the data via a graph. Here, in this case, the data is already in integer format so nothing to worry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qPbKQ0noRptD",
    "outputId": "6a5aea47-ad0c-4118-8471-d91b6432b339",
    "papermill": {
     "duration": 0.040549,
     "end_time": "2020-11-27T11:52:30.738002",
     "exception": false,
     "start_time": "2020-11-27T11:52:30.697453",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather basic information about the dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather descriptive statistics about the data\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the size of the dataframe\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "beRJyQAezdX8",
    "papermill": {
     "duration": 0.029309,
     "end_time": "2020-11-27T11:52:30.797064",
     "exception": false,
     "start_time": "2020-11-27T11:52:30.767755",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Out-of-the box more advanced EDA with ydata profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ydata_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = ProfileReport(df, title=\"Cars Dataset Profiling Report\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QoHuBhXxT5E9",
    "papermill": {
     "duration": 0.029086,
     "end_time": "2020-11-27T11:52:30.855758",
     "exception": false,
     "start_time": "2020-11-27T11:52:30.826672",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5. Dropping irrelevant columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_3cy877Mze4H",
    "papermill": {
     "duration": 0.029217,
     "end_time": "2020-11-27T11:52:30.914753",
     "exception": false,
     "start_time": "2020-11-27T11:52:30.885536",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This step is certainly needed in every EDA because sometimes there would be many columns that we never use in such cases dropping is the only solution. In this case, the columns such as Engine Fuel Type, Market Category, Vehicle style, Popularity, Number of doors, Vehicle Size doesn't make any sense to me so I just dropped for this instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uvSkK8swTr9H",
    "outputId": "1734f538-a4a1-45b9-d656-7ddc0124dc35",
    "papermill": {
     "duration": 0.053265,
     "end_time": "2020-11-27T11:52:30.997635",
     "exception": false,
     "start_time": "2020-11-27T11:52:30.944370",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.drop(['Engine Fuel Type', 'Market Category', 'Vehicle Style', 'Popularity', 'Number of Doors', 'Vehicle Size'], axis=1)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20OeQBpWz89v",
    "papermill": {
     "duration": 0.029929,
     "end_time": "2020-11-27T11:52:31.057925",
     "exception": false,
     "start_time": "2020-11-27T11:52:31.027996",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "caAownWdUZso",
    "papermill": {
     "duration": 0.030223,
     "end_time": "2020-11-27T11:52:31.118132",
     "exception": false,
     "start_time": "2020-11-27T11:52:31.087909",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 6. Renaming the columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UABLiEcyz-2G",
    "papermill": {
     "duration": 0.030318,
     "end_time": "2020-11-27T11:52:31.178829",
     "exception": false,
     "start_time": "2020-11-27T11:52:31.148511",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In this instance, most of the column names are very confusing to read, so I just tweaked their column names. This is a good approach it improves the readability of the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1wHW41whURub",
    "outputId": "dec9b1b3-e344-4b33-92fd-6e9f4c03c878",
    "papermill": {
     "duration": 0.05128,
     "end_time": "2020-11-27T11:52:31.260584",
     "exception": false,
     "start_time": "2020-11-27T11:52:31.209304",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.rename(columns={\"Engine HP\": \"HP\", \"Engine Cylinders\": \"Cylinders\", \"Transmission Type\": \"Transmission\", \"Driven_Wheels\": \"Drive Mode\",\"highway MPG\": \"MPG-H\", \"city mpg\": \"MPG-C\", \"MSRP\": \"Price\" })\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5tcGiOmV0afN",
    "papermill": {
     "duration": 0.030645,
     "end_time": "2020-11-27T11:52:31.323355",
     "exception": false,
     "start_time": "2020-11-27T11:52:31.292710",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uiW7x_O4WIDX",
    "papermill": {
     "duration": 0.030638,
     "end_time": "2020-11-27T11:52:31.385353",
     "exception": false,
     "start_time": "2020-11-27T11:52:31.354715",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 7. Duplicated Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Identifying duplicates** \n",
    "A simple way to identify duplicates is to use the  `pd.duplicated()` function from Pandas. This function returns a series of “true/false” outputs, with “true” indicating the data value is a duplicate, and “false” indicating it is a unique value.\n",
    "\n",
    "**Keeping or Dropping Duplicates** Every dataset is unique and you cannot treat every dataset the same. When making the decision on whether to eliminate duplicate values or not, think deeply about the dataset itself and about the objective you wish to achieve. What impact will dropping duplicates have on your dataset and your objective? \n",
    "1. **Deciding to drop |** You should drop or eliminate duplicate values if duplicate values are clearly mistakes or will misrepresent the remaining unique values in the dataset.  \n",
    "2. **Deciding to NOT drop |** You should keep duplicated data in your dataset if the duplicate values are clearly not mistakes and should be taken into account when representing the dataset as a whole. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9LpR5NW70hXm",
    "papermill": {
     "duration": 0.030478,
     "end_time": "2020-11-27T11:52:31.446542",
     "exception": false,
     "start_time": "2020-11-27T11:52:31.416064",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Dropping duplicates is often a handy thing to do because a huge data set as in this case contains more than 10, 000 rows often have some duplicate data which might be disturbing, so here I remove all the duplicate value from the data-set. For example prior to removing I had 11914 rows of data but after removing the duplicates 10925 data meaning that I had 989 of duplicate data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1nBN_BCDWSmv",
    "outputId": "9a070a7d-a4d4-45c7-cac8-acb2c4db0e72",
    "papermill": {
     "duration": 0.039307,
     "end_time": "2020-11-27T11:52:31.516612",
     "exception": false,
     "start_time": "2020-11-27T11:52:31.477305",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yB8t6o0wH7If",
    "outputId": "d4778cd1-5372-4e27-db62-20635493786e",
    "papermill": {
     "duration": 0.053706,
     "end_time": "2020-11-27T11:52:31.602271",
     "exception": false,
     "start_time": "2020-11-27T11:52:31.548565",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "duplicate_rows_df = df[df.duplicated()]\n",
    "print(\"number of duplicate rows: \", duplicate_rows_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "chhNvMFCIzqI",
    "papermill": {
     "duration": 0.031075,
     "end_time": "2020-11-27T11:52:31.664844",
     "exception": false,
     "start_time": "2020-11-27T11:52:31.633769",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now let us remove the duplicate data because it's ok to remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kuobmetTV820",
    "outputId": "c6d6ce5c-6a38-4cd2-ee99-151124a1f84d",
    "papermill": {
     "duration": 0.045289,
     "end_time": "2020-11-27T11:52:31.741503",
     "exception": false,
     "start_time": "2020-11-27T11:52:31.696214",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.count()      # Used to count the number of rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_MJKjbzHI40K",
    "papermill": {
     "duration": 0.032123,
     "end_time": "2020-11-27T11:52:31.806242",
     "exception": false,
     "start_time": "2020-11-27T11:52:31.774119",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "So seen above there are 11914 rows and we are removing 989 rows of duplicate data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OiOsEF6WVTSj",
    "outputId": "a527c1a2-5d74-42bb-99e2-8112ebffb871",
    "papermill": {
     "duration": 0.058104,
     "end_time": "2020-11-27T11:52:31.896493",
     "exception": false,
     "start_time": "2020-11-27T11:52:31.838389",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2gMM4lb0Vzor",
    "outputId": "f04e1803-e7de-4cbf-fdeb-8449a8051a07",
    "papermill": {
     "duration": 0.045247,
     "end_time": "2020-11-27T11:52:31.974315",
     "exception": false,
     "start_time": "2020-11-27T11:52:31.929068",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zCLUdZOQ1PDP",
    "papermill": {
     "duration": 0.032655,
     "end_time": "2020-11-27T11:52:32.040143",
     "exception": false,
     "start_time": "2020-11-27T11:52:32.007488",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JkXUQtyQW3Dy",
    "papermill": {
     "duration": 0.033049,
     "end_time": "2020-11-27T11:52:32.106118",
     "exception": false,
     "start_time": "2020-11-27T11:52:32.073069",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 8. Dropping the missing or null values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K5pKvJi41YCp",
    "papermill": {
     "duration": 0.033171,
     "end_time": "2020-11-27T11:52:32.173467",
     "exception": false,
     "start_time": "2020-11-27T11:52:32.140296",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This is mostly similar to the previous step but in here all the missing values are detected and are dropped later. Now, this is not a good approach to do so, because many people just replace the missing values with the mean or the average of that column, but in this case, I just dropped that missing values. This is because there is nearly 100 missing value compared to 10, 000 values this is a small number and this is negligible so I just dropped those values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tzdlg-1OWjMz",
    "outputId": "7375ab6e-1473-4346-e5b5-0c61189cc716",
    "papermill": {
     "duration": 0.045703,
     "end_time": "2020-11-27T11:52:32.252054",
     "exception": false,
     "start_time": "2020-11-27T11:52:32.206351",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iWJqTVxTJQnO",
    "papermill": {
     "duration": 0.033238,
     "end_time": "2020-11-27T11:52:32.318801",
     "exception": false,
     "start_time": "2020-11-27T11:52:32.285563",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This is the reason in the above step while counting both Cylinders and Horsepower (HP) had 10856 and 10895 over 10925 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KbbV0xHPWoad",
    "outputId": "17dda8ec-1282-4814-de79-8f5e1aff3a5f",
    "papermill": {
     "duration": 0.071586,
     "end_time": "2020-11-27T11:52:32.424366",
     "exception": false,
     "start_time": "2020-11-27T11:52:32.352780",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.dropna()    # Dropping the missing values.\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2t7L9l2mJSoX",
    "papermill": {
     "duration": 0.033726,
     "end_time": "2020-11-27T11:52:32.496715",
     "exception": false,
     "start_time": "2020-11-27T11:52:32.462989",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now we have removed all the rows which contain the Null or N/A values (Cylinders and Horsepower (HP))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V-DmX1O4Wtox",
    "outputId": "2d50fc20-3535-413b-e317-75a7f94fa2a2",
    "papermill": {
     "duration": 0.045646,
     "end_time": "2020-11-27T11:52:32.576197",
     "exception": false,
     "start_time": "2020-11-27T11:52:32.530551",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(df.isnull().sum())   # After dropping the values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bk8RAHqQJVJK",
    "papermill": {
     "duration": 0.033656,
     "end_time": "2020-11-27T11:52:32.644093",
     "exception": false,
     "start_time": "2020-11-27T11:52:32.610437",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Py3sQc_ZxyU",
    "papermill": {
     "duration": 0.034018,
     "end_time": "2020-11-27T11:52:32.712147",
     "exception": false,
     "start_time": "2020-11-27T11:52:32.678129",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 9. Detecting Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outliers |** Observations that are an abnormal distance from other values or an overall pattern in a data population\n",
    "\n",
    "**3 Types of Outliers**\n",
    "- Global outliers \n",
    "- Contextual outliers\n",
    "- Collective outliers\n",
    "\n",
    "**Global Outliers |** Values that are completely different from the overall data group and have noa association with any other outliers\n",
    "\n",
    "**Contextual outliers |** Normal data points under certain conditions but become anomalies under most other conditions \n",
    "\n",
    "**Collective outliers |** A group of abnormal point that follow similar patterns and are isolated from the rest of the population \n",
    "\n",
    "**How to handle outliers** \n",
    "\n",
    "It is important to not only detect outliers, but also to have a plan for them.\n",
    "\n",
    "Whether you keep outliers as they are, delete them, or reassign values is a decision that you make on a dataset-by-dataset basis. To help you make the decision, you can start with these general guidelines:\n",
    "\n",
    "- **Delete them**: If you are sure the outliers are mistakes, typos, or errors and the dataset will be used for modeling or machine learning, then you are more likely to decide to delete outliers. Of the three choices, you’ll use this one the least.\n",
    "- **Reassign them**: If the dataset is small and/or the data will be used for modeling or machine learning, you are more likely to choose a path of deriving new values to replace the outlier values.\n",
    "- **Leave them**: For a dataset that you plan to do EDA/analysis on and nothing else, or for a dataset you are preparing for a model that is resistant to outliers, it is most likely that you are going to leave them in.\n",
    "\n",
    "**Useful Functions:**\n",
    "\n",
    "| Function | Description |\n",
    "| ---- | ---- |\n",
    "`df.describe()` | A DataFrame method that returns general statistics about the dataframe which can help determine outliers |\n",
    "`sns.boxplot()` | A seaborn function that generates a box plot. Data points beyond 1.5x the interquartile range are considered outliers. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vtxX_y6zZ2ri",
    "outputId": "e3f93522-9244-4c32-c34c-103e0834e93b",
    "papermill": {
     "duration": 0.283132,
     "end_time": "2020-11-27T11:52:33.097091",
     "exception": false,
     "start_time": "2020-11-27T11:52:32.813959",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.boxplot(x=df['Price'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9HtvQYVHZ8u5",
    "outputId": "3dc30a01-6fb1-41d9-dec8-0ceeead6a358",
    "papermill": {
     "duration": 0.237157,
     "end_time": "2020-11-27T11:52:33.370221",
     "exception": false,
     "start_time": "2020-11-27T11:52:33.133064",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.boxplot(x=df['HP'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xz3MwIjbaBUr",
    "outputId": "335d1e55-55a3-4e61-8401-a414580d9d62",
    "papermill": {
     "duration": 0.262939,
     "end_time": "2020-11-27T11:52:33.670253",
     "exception": false,
     "start_time": "2020-11-27T11:52:33.407314",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.boxplot(x=df['Cylinders'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only numerical columns from the DataFrame\n",
    "numerical_df = df.select_dtypes(include=['number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cFG9hck7aHUx",
    "outputId": "7c09ff98-3725-4d56-9b97-ef6b43d0e17b",
    "papermill": {
     "duration": 0.051549,
     "end_time": "2020-11-27T11:52:33.759385",
     "exception": false,
     "start_time": "2020-11-27T11:52:33.707836",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Q1 = numerical_df.quantile(0.25)\n",
    "Q3 = numerical_df.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "print(IQR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kCv110_cJiDz",
    "papermill": {
     "duration": 0.036905,
     "end_time": "2020-11-27T11:52:33.833569",
     "exception": false,
     "start_time": "2020-11-27T11:52:33.796664",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Don't worry about the above values because it's not important to know each and every one of them because it's just important to know how to use this technique in order to remove the outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "igh_mRXeaJrI",
    "outputId": "44660b71-2eb7-4387-c599-e527640a7c21",
    "papermill": {
     "duration": 0.07026,
     "end_time": "2020-11-27T11:52:33.941373",
     "exception": false,
     "start_time": "2020-11-27T11:52:33.871113",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filter the DataFrame to remove outliers\n",
    "df_filtered = numerical_df[~((numerical_df < (Q1 - 1.5 * IQR)) | (numerical_df > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "\n",
    "# Check the new shape\n",
    "df_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the original DataFrame using the index of filtered rows\n",
    "df = df.loc[df_filtered.index]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Nh93DCGJkqT",
    "papermill": {
     "duration": 0.037923,
     "end_time": "2020-11-27T11:52:34.017297",
     "exception": false,
     "start_time": "2020-11-27T11:52:33.979374",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "As seen above there were around 1600 rows were outliers. But you cannot completely remove the outliers because even after you use the above technique there maybe 1–2 outlier unremoved but that ok because there were more than 100 outliers. Something is better than nothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z87kHbgvaQbb",
    "papermill": {
     "duration": 0.037301,
     "end_time": "2020-11-27T11:52:34.092399",
     "exception": false,
     "start_time": "2020-11-27T11:52:34.055098",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WqFPRda8eEp_",
    "papermill": {
     "duration": 0.038833,
     "end_time": "2020-11-27T11:52:34.169033",
     "exception": false,
     "start_time": "2020-11-27T11:52:34.130200",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 10. Plot different features against one another (scatter), against frequency (histogram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z-W6Q9-hJosZ",
    "papermill": {
     "duration": 0.037659,
     "end_time": "2020-11-27T11:52:34.244690",
     "exception": false,
     "start_time": "2020-11-27T11:52:34.207031",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Histogram\n",
    "\n",
    "Histogram refers to the frequency of occurrence of variables in an interval. In this case, there are mainly 10 different types of car manufacturing companies, but it is often important to know who has the most number of cars. To do this histogram is one of the trivial solutions which lets us know the total number of car manufactured by a different company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dAnd4DSyeHDb",
    "outputId": "44b04e39-9dc7-40fc-9ddb-b7182f4f6e1f",
    "papermill": {
     "duration": 0.645152,
     "end_time": "2020-11-27T11:52:34.927853",
     "exception": false,
     "start_time": "2020-11-27T11:52:34.282701",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.Make.value_counts().nlargest(40).plot(kind='bar', figsize=(10,5))\n",
    "plt.title(\"Number of cars by make\")\n",
    "plt.ylabel('Number of cars')\n",
    "plt.xlabel('Make');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c37WtYYWJuAQ",
    "papermill": {
     "duration": 0.04046,
     "end_time": "2020-11-27T11:52:35.081261",
     "exception": false,
     "start_time": "2020-11-27T11:52:35.040801",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Heat Maps\n",
    "\n",
    "Heat Maps is a type of plot which is necessary when we need to find the dependent variables. One of the best way to find the relationship between the features can be done using heat maps. In the below heat map we know that the price feature depends mainly on the Engine Size, Horsepower, and Cylinders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yhHfLVTj6nmy",
    "outputId": "50a65ae0-841a-42ec-87e6-1a99da1ea57b",
    "papermill": {
     "duration": 0.419422,
     "end_time": "2020-11-27T11:52:35.541316",
     "exception": false,
     "start_time": "2020-11-27T11:52:35.121894",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "c= numerical_df.corr()\n",
    "sns.heatmap(c,cmap=\"BrBG\",annot=True)\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ROThOLQfRZw",
    "papermill": {
     "duration": 0.041995,
     "end_time": "2020-11-27T11:52:35.626197",
     "exception": false,
     "start_time": "2020-11-27T11:52:35.584202",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Scatterplot\n",
    "\n",
    "We generally use scatter plots to find the correlation between two variables. Here the scatter plots are plotted between Horsepower and Price and we can see the plot below. With the plot given below, we can easily draw a trend line. These features provide a good scattering of points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2aOfHNFefSrX",
    "outputId": "4b5286f0-5419-48bc-d2de-476aeb36f022",
    "papermill": {
     "duration": 0.319087,
     "end_time": "2020-11-27T11:52:35.987616",
     "exception": false,
     "start_time": "2020-11-27T11:52:35.668529",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "ax.scatter(df['HP'], df['Price'])\n",
    "ax.set_xlabel('HP')\n",
    "ax.set_ylabel('Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the data does not contain NaN values\n",
    "df = df.dropna(subset=['HP', 'Price'])\n",
    "\n",
    "# Extract the relevant columns\n",
    "x = df['HP']\n",
    "y = df['Price']\n",
    "\n",
    "# Calculate the coefficients for the trend line (linear regression)\n",
    "coefficients = np.polyfit(x, y, 1)  # Degree 1 for a linear trend\n",
    "trend_line = np.poly1d(coefficients)\n",
    "\n",
    "# Generate the y-values for the trend line\n",
    "trend_y = trend_line(x)\n",
    "\n",
    "# Plot the scatter plot with the trend line\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.scatter(x, y, alpha=0.7, label='Data Points')\n",
    "ax.plot(x, trend_y, color='red', label='Trend Line', linewidth=2)\n",
    "ax.set_xlabel('HP')\n",
    "ax.set_ylabel('Price')\n",
    "ax.set_title('Scatter Plot of HP vs Price with Trend Line')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hQXy8o_gKFS5",
    "papermill": {
     "duration": 0.043811,
     "end_time": "2020-11-27T11:52:36.075327",
     "exception": false,
     "start_time": "2020-11-27T11:52:36.031516",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Hence the above are some of the steps involved in Exploratory data analysis, these are some general steps that you must follow in order to perform EDA. There are many more yet to come but for now, this is more than enough idea as to how to perform a good EDA given any data sets. Stay tuned for more updates.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "papermill": {
   "duration": 12.885345,
   "end_time": "2020-11-27T11:52:37.154623",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-11-27T11:52:24.269278",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
